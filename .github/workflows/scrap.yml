name: GScrap

on:
  workflow_dispatch:  # manual trigger
  schedule:
    - cron: "0 */6 * * *"  # every 6 hours

jobs:
  scrape_emails:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout the repository
      - name: Checkout repository
        uses: actions/checkout@v2

      # Step 2: Set up Python environment
      - name: Set up Python 3.9
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'

      # Step 3: Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Step 4: Decode Google Sheets credentials
      - name: Decode Google Sheets credentials
        run: echo "${{ secrets.ENCODED_GOOGLE_SHEET_CREDENTIALS }}" | base64 --decode > credentials.json

      # Step 5: Run the email scraper script
      - name: Run email scraper script
        env:
          GITHUB_API_KEYS: ${{ secrets.GITHUB_API_KEYS }}  # GitHub API keys should be stored in GitHub Secrets
        run: |
          python CloudScrap.py
